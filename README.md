## ğŸª¨ï¸ Making Retrieval-Augmented Language Models Robust to Irrelevant Context

### RetRobust Overview
By training RALMs on 1K examples we can make them robust to irrelevant context and improve QA performance
([**Paper**](?)).

![Alt text](images/retrobust_fig_1.png?raw=true "Retrobust examples")


###  ğŸ¤— Data and Models
Our models and data are available at the [**RetRobust HuggingFace Collection**](https://huggingface.co/collections/Ori/retrobust-65198eef2b4fffcb4100e163).

### ğŸ§—ğŸ½ Experiments framework
LLama-2 inference servers were set using [**lm-sys/FastChat**](https://github.com/lm-sys/FastChat). Experiments were run using the framework from [**reasoning-on-cots**](https://github.com/oriyor/reasoning-on-cots). More details coming soon...

### âœ Citation
```
bibtex
@article{
}
```