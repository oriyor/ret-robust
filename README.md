## ğŸª¨ï¸ Making Retrieval-Augmented Language Models Robust to Irrelevant Context

### RetRobust Overview

Augmenting RALMs with irrelevant context can significantly decrease performance.
We can robustify them by training on 1K examples and increase QA accuracy
([**Paper**](?)).

![Alt text](images/retrobust_fig_1.png?raw=true "Retrobust examples")

### ğŸ§—ğŸ½ Experiments framework
LLama-2 inference servers were set using [**lm-sys/FastChat**](https://github.com/lm-sys/FastChat). Experiments were run using the framework from the [**Multi-Chain Reasoning Paper**](https://github.com/oriyor/reasoning-on-cots).

###  ğŸ¤— Data and Models
Our models are publicly available at the [**RetRobust HuggingFace Collection**](https://huggingface.co/collections/Ori/retrobust-65198eef2b4fffcb4100e163).
More details coming soon...

### âœ Citation
```
bibtex
@article{
}
```