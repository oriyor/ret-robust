## 🪨️ Making Retrieval-Augmented Language Models Robust to Irrelevant Context

### RetRobust Overview
By training RALMs on 1K examples we can make them robust to irrelevant context and improve QA performance
([**Paper**](?)).

![Alt text](images/retrobust_fig_1.png?raw=true "Retrobust examples")

### 🧗🏽 Experiments framework
LLama-2 inference servers were set using [**lm-sys/FastChat**](https://github.com/lm-sys/FastChat). Experiments were run using the framework from the [**Multi-Chain Reasoning Paper**](https://github.com/oriyor/reasoning-on-cots).

###  🤗 Data and Models
Our models are publicly available at the [**RetRobust HuggingFace Collection**](https://huggingface.co/collections/Ori/retrobust-65198eef2b4fffcb4100e163).
More details coming soon...

### ✍ Citation
```
bibtex
@article{
}
```